{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d76bcfc-01ee-4b7d-befa-721e93214bb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook: Step 3 – Train, Evaluate, and Log Model (Unity Catalog)\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from mlflow.models.signature import infer_signature\n",
    "from pyspark.sql.functions import col\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 1. Load engineered features\n",
    "# --------------------------------------------------\n",
    "data = spark.read.table(\"default.features_credit_train\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 2. Split into train/test\n",
    "# --------------------------------------------------\n",
    "train_df, test_df = data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 3. Train model\n",
    "# --------------------------------------------------\n",
    "rf = RandomForestClassifier(\n",
    "    labelCol=\"default_flag\",\n",
    "    featuresCol=\"features\",\n",
    "    numTrees=100,\n",
    "    maxDepth=8,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 4. Configure MLflow (Unity Catalog)\n",
    "# --------------------------------------------------\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "mlflow.set_experiment(\"/Shared/banking_credit_default_experiment\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"rf_baseline_train_eval\") as run:\n",
    "    model = rf.fit(train_df)\n",
    "    preds = model.transform(test_df)\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 5. Evaluate metrics\n",
    "    # --------------------------------------------------\n",
    "    evaluator_auc = BinaryClassificationEvaluator(labelCol=\"default_flag\", metricName=\"areaUnderROC\")\n",
    "    evaluator_acc = MulticlassClassificationEvaluator(labelCol=\"default_flag\", metricName=\"accuracy\")\n",
    "    evaluator_f1  = MulticlassClassificationEvaluator(labelCol=\"default_flag\", metricName=\"f1\")\n",
    "\n",
    "    auc = evaluator_auc.evaluate(preds)\n",
    "    acc = evaluator_acc.evaluate(preds)\n",
    "    f1  = evaluator_f1.evaluate(preds)\n",
    "\n",
    "    print(f\"ROC-AUC: {auc:.4f} | Accuracy: {acc:.4f} | F1: {f1:.4f}\")\n",
    "\n",
    "    mlflow.log_params({\"num_trees\": 100, \"max_depth\": 8})\n",
    "    mlflow.log_metrics({\"roc_auc\": auc, \"accuracy\": acc, \"f1\": f1})\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 6. Prepare model signature and input example\n",
    "    # --------------------------------------------------\n",
    "    # Skip vector column for JSON serialization\n",
    "    feature_cols = [c for c in train_df.columns if c not in [\"features\", \"default_flag\"]]\n",
    "\n",
    "    sample_input = train_df.select(*feature_cols).limit(5).toPandas()\n",
    "    sample_output = model.transform(train_df.limit(5)).select(\"prediction\").limit(5).toPandas()\n",
    "\n",
    "    signature = infer_signature(sample_input, sample_output)\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # 7. Log model to MLflow (UC volume temp path)\n",
    "    # --------------------------------------------------\n",
    "    mlflow.spark.log_model(\n",
    "        spark_model=model,\n",
    "        artifact_path=\"model\",\n",
    "        dfs_tmpdir=\"/Volumes/banking/default/mlflow_tmp\",\n",
    "        signature=signature,\n",
    "        input_example=sample_input\n",
    "    )\n",
    "\n",
    "    print(f\"Run ID: {run.info.run_id}\")\n",
    "    mlflow.log_text(run.info.run_id, \"run_id.txt\")\n",
    "\n",
    "print(\"✅ Step 3 complete – model, signature, and metrics logged to MLflow (Unity Catalog).\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Train_and_evaluate",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
